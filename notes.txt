Feature engineering
-------------------
Would like to derive variables e.g. density, momentum from known data
See load-grid-info.py to get pressures at altitude levels

# Altitude levels in hPa from ClimSim-main\grid_info\ClimSim_low-res_grid-info.nc
level_pressure_hpa = [0.07834781133863082, 0.1411083184744011, 0.2529232969453412, 0.4492506351686618, 0.7863461614709879, 1.3473557602677517, 2.244777286900205, 3.6164314830257718, 5.615836425337344, 8.403253219853443, 12.144489352066294, 17.016828024303006, 23.21079811610005, 30.914346261995327, 40.277580662953575, 51.37463234765765, 64.18922841394662, 78.63965761131159, 94.63009200213703, 112.09127353988006, 130.97780378937776, 151.22131809551237, 172.67390465199267, 195.08770981962772, 218.15593476138105, 241.60037901222947, 265.2585152868483, 289.12232222921756, 313.31208711045167, 338.0069992368819, 363.37349177951705, 389.5233382784413, 416.5079218282233, 444.3314120123719, 472.9572063769364, 502.2919169181905, 532.1522731583445, 562.2393924639011, 592.1492760575118, 621.4328411158061, 649.689897132655, 676.6564846051039, 702.2421877859194, 726.4985894989197, 749.5376452869328, 771.4452171682528, 792.2342599534793, 811.8566751313328, 830.2596431972574, 847.4506530638328, 863.5359020075301, 878.7158746040692, 893.2460179738746, 907.3852125876941, 921.3543974831824, 935.3167171670306, 949.3780562075774, 963.5995994020714, 978.013432382012, 992.6355435925217]

Ideas to progress
-----------------

o Bug if test only currently does validation loops
o Logarithmic scaling so that reasonable values near 1, extremes not so extreme
o Went badly deleting hopefully redundant features at b32db7d49, so what is so crucial?
o Analyse which columns contributing most to error metric:
   - ptend_q0001 moistening tendency low indices >=12, then higher ~45
   - ptend_q0002 cloud liquid/ice mixing ratios and ptend_q0002 ice mixing ratios
   -- both to do with condensation/evaporation
   -- mass mixing ratio is m(ice/water) / m(dry air) 
o Prev expts where knocked out half featurs etc bogus because before new ones added
o Would I do better by just zeroing/averaging some output columns with poor R2?
  - Should be able to analyse without retraining on best model, extending R2
    analysis to all validation set and checking if substitute with avg of training
    output col. But beware train/val overlap if random split not reproducible.
  - But existing R2 analysis had all R2 > 0, whereas mean would only score 0.
o Haven't explored optimiser etc (but did learning rate)
o Deal properly with columns that are going to be zero anyway
o Tried float64 on 1M samples, very slow and convergence oddly seemed worse
o Understand physics of each target col better
o Does humidity have significant effect on heat capacity? And cloud/rain/snow content?
o Deeper/alternative network structures
o Try other ML techniques apart from NN
 - Use R2 scores from catboost and CNN to decide weighting for each output?
 - q0001 (moistening) and q0002 (cloud liquid ratio) worst from CNN at present but 
   all could do with some help
o Ensemble method combining independent technique(s) and NN
o Would it help to remove more old feature columns? Could automate experiment
  - brief expt removed temperature, bit worse
o Would it help to group columns differently depending on technique?
o Train/val split does appear deterministic (random seeds set)


Scoring and why predictions zeroed for low-variance cols in example
-------------------------------------------------------------------

Uses sklearn.metrics.r2_score
Competition links to https://www.kaggle.com/code/jerrylin96/r2-score-default
Discussion https://www.kaggle.com/competitions/leap-atmospheric-physics-ai-climsim/discussion/495255
R2 is average across R2 for individual columns, but with weighting
R2 for one col = 1 - ( (sq resids sum of prediction) / ((true - avg true) sq sum))
So get R2=0 if just guess mean for column
If true values have very low variance, (true - avg true) -> 0 and R2 -> inf
R2=0 a lot better than wild value if a little bit off for low-variance columns, hence
guessing mean like sample code


Beucler et al
o Transformed relative humidity to specific humidity for much better generalisation
  https://colab.research.google.com/github/tbeucler/CBRAIN-CAM/blob/master/Climate_Invariant_Guide.ipynb
o Transformed temperature to a buoyancy metric, also improved but only with RH too
  See "Source code for the moist thermodynamics library" cell, class T2BMSENumpy:
  https://colab.research.google.com/github/tbeucler/CBRAIN-CAM/blob/master/Climate_Invariant_Guide.ipynb#scrollTo=0S6W988UaG6p
o Radiative effects: absorbtion of SW (visible), greenhouse trapping of LW (IR)
o Sensible heat flux: +ve from atm to surface (radiation, conduction, convection??)
o Latent heat flux: +ve for condensation on surface, -ve for evaporation from surface

Performance
-----------

At a17aae4 on greta scaling and model saved at 03:10 first cache 11:55 so ~3hr15 for 20 epochs, batch size 30000
On pcuk151 first cache 21:12, epoch 10 finishing 05:50 so ~8hr40 for 10 epochs, batch size 5000
Though validation loss looking better so far on pcuk151, maybe batches too big?

Found layer normalisation and SiLU best in experiment 26May2024 though little in it

Submission Results
------------------

submission.csv
Complete · 5m ago · Commit b6c560a9 CNN big run but 20 epochs old weightings, to 31 epochs with new weightings hoping would transfer, R2 locally looks terrible though validation loss OK
-392.89850

Hmm don't know what went so terribly wrong here...
submission.csv
Complete · 3m ago · Commit b6c560a9 switched to new submission weights for only last 4 of 20 epochs, baseline CNN rerun to check not broken after refactor
-30331453942559299857621134082048.00000

averaged_submission.csv
Complete · 5m ago · Commit 15a6f84 Experimentally averaging 5 individual submission dataframes over first 500K rows to compare with catboost model sum over same data
0.37763

submission.csv
Complete · 26m ago · Commit 02a87b6 catboost over 500K rows but data normalised over 5M
0.39272

submission.csv
Complete · 23m ago · Commit 718c237 catboost over first 100K rows but data normalised over 5M
0.35673

submission.csv
Complete · 4m ago · Commit c6093c7b best catboost params on 2M [actually 200K] rows but looks worse than 100K expts for some reason
0.36803

submission.csv -- doing better with more iterations and higher learning rate
Complete · 4h ago · Commit 63d2cf47 multitrain hyperparam search with catboost, ended using model_border_count_16_depth_8_iterations_20_learning_rate_0.1_l2_leaf_reg_2.pkl
0.30880

submission.csv -- hmm slightly worse doing 10M not 1M training rows... many failed R2 test in fact.
Complete · 4m ago · Commit f2bc0a9 all-rows catboost run after R2<=0 fix
0.22795

submission.csv
Complete · 2m ago · Commit 0d79ad2 catboost test run with 1M rows after R2<=0 exclusion bugfix
0.23663

Oops what went wrong with this?
submission.csv
Complete · 3m ago · Commit 7190f38 catboost with more features and all training rows, substituting mean where R2 <= 0
-875470.00698

submission.csv
Complete · 6m ago · Commit 0b349a25 first catboost test with only 10000 rows
0.17413

submission.csv
Complete · 3m ago · Commit 20fe5c45b trying a simpler feature set and model on a hunch in case converged better, still with poly output layer, but val loss doesn't look good
0.59769

submission.csv
Complete · 2h ago · Commit f432f9e poly output again but poor val loss
0.62373 (better than I expected)

submission.csv
Complete · 7m ago · Commit 18298fd polynomial output to try and reach outlier values but didn't converge as well as in smaller run
0.55093

Better validation loss here, but no better in competition:
submission.csv
Complete · 2m ago · Commit d138788 with cloud mass and rethought GWP features 26 epochs final val loss ~0.275
0.66457

submission.csv
Complete · 4m ago · Commit 72fb1b5 continued now with uniform scaling across atmospheric levels
0.67054

submission.csv
Complete · 1d ago · Commit d0733e88 no y norm offset, a few cols now nixed
0.66888

submission.csv
Complete · 6m ago · Commit 77a04229 with integral cloud features
0.65123
Ran until patience counter of 3 exhausted, 35 epochs validation loss 0.31098 best

submission.csv
Complete · 6m ago · Commit b1472343 with corrected width 1-deep initial conv
Ran until patience counter of 3 exhausted, 42 epochs validation loss 0.31218 beset
0.65009

submission.csv
Complete · 4m ago · Commit e352323c with new scalar/vector output structure and bigger conv depth, 23 epochs
0.64229

submission.csv
Complete · 2h ago · Commit e77e89d8 first automated model param search, only 10 epochs, best of 9 was model.pt_gen_conv_width_7_gen_conv_depth_12.pt
In general depth in conv layers was dominant, 12 better than 6 better than 3.
Width across atm layers less important, but probably 7/15 better than 3.
But also a lot more parameters and computing time.
0.61821


submission.csv
Complete · 19m ago · Commit 939c2b5 reverting to fewer but deeper conv layers, 5000 batches, with buoyancy feature
0.62494

submission.csv
Complete · 5m ago · Commit b8a8d4ec 4th conv layer all depth 3, 30000s
0.60257

submission-bd3a254.csv
Complete · 5m ago · Commit bd3a254 more depth in conv layers, 5000-row batches
0.62144 (notably better than larger batches this time at least)

submission.csv
Complete · 9h ago · Commit a17aae411 deeper conv layers 30000 batches
0.60331

submission-1b655d19.csv
Complete · 2h ago · Commit 1b655d19 with experimental no centring of x values in normalisation either
0.59778 -- so a bit worse overall, will centre x values again


submission.csv
Complete · 2m ago · commit 9ccabc17 not centering Y values in normalisation
0.60646

submission-5a5069fa.csv
Complete · 6m ago · Commit 5a5069fa 10 epochs with high dropout proportion, validation loss not good though
0.55404


Tried LR=0.01 but didn't really converge:
	Epoch 1, Step 610, Training Loss: 0.6596
With LR=0.004 seemed very smooth, actually slower than 0.001:
Epoch 2, Step 890, Training Loss: 0.4715
Trying now with LR=0.0004 on Kaggle for 3 epochs:
Epoch 3, Validation Loss: 0.4196762824707692 so no improvement
So default learning rate of 0.001 actually good with current setup.


commit 25b97f1f Fix to apparently incorrect y scaling
Oddly didn't help, maybe worse -- maybe means scaling alone is fine, no need
to subtract mean at all, which would save time.
Epoch 10, Validation Loss: 0.38333882804554287 (was 0.367 below before fix)
Epoch 13, Validation Loss: 0.38022170975656794 + 3 epochs for sanity check submit
Epoch 15, Validation Loss: 0.3789655051314005
Epoch 18, Validation Loss: 0.37621909586509855
Did end up with best score so far, but a lot of epochs:
submission-25b97f1f-23-epochs.csv
Complete · 2h ago · commit 25b97f1f With y scaling fixed (but didn't help?)
0.59192

submission-559738c.csv
Complete · 3m ago · Normalising inputs (but not yet outputs) across whole channels
0.58698
Epoch 1, Validation Loss: 0.43317776520063384
Epoch 2, Validation Loss: 0.41231624118172294
Epoch 3, Validation Loss: 0.39903453848149517
Epoch 4, Validation Loss: 0.3914585606296464
Epoch 5, Validation Loss: 0.38657170696423787 
Epoch 6, Validation Loss: 0.38171850779269
Epoch 7, Validation Loss: 0.3780829362054863
Epoch 8, Validation Loss: 0.3759397563367787
Epoch 9, Validation Loss: 0.37318830088813704
Epoch 10, Validation Loss: 0.3713240346105972
Epoch 11, Validation Loss: 0.36997252392886887
Epoch 12, Validation Loss: 0.36848068200420625
Epoch 13, Validation Loss: 0.3670755941236373

submission-763ae6b9.csv
Complete · 4m ago · Commit 763ae6b9 Trying more channels in early CNN layers
0.57444
Epoch 8, Validation Loss: 0.3771515665668072
Epoch 9, Validation Loss: 0.37443682448108595
Epoch 10, Validation Loss: 0.37198687337412695


submission_8531e050.csv
Complete · 6m ago · Commit 8531e050 with more droput and layer normalisations
Epoch 19, Validation Loss: 0.38724345529433524 (full dataset)
0.56218

submission.csv
Complete · 2m ago · Commit 77a26e95b9a local run to validation score ~0.4 on full dataset with CNN
0.53044

LEAP with feature engineering - Version 35
Complete · 3m ago · Commit 2c9a279 smallish test run with CNN approach and overhauled test output
With only: Validation Loss: 0.5902572572231293
0.35463

LEAP with feature engineering - Version 25
Complete · 3d ago · commit 755728af first test run with holo frame concept
0.28431

LEAP with feature engineering - Version 19
Complete · 4m ago · commit afda421d20 Multiplying by submission weights before modelling now
0.32041

LEAP with feature engineering - Version 18
Complete · 19h ago · commit c7b0cfa660 Understood R2 measure now and some sense of why sample code was zeroing (actually setting mean) for columns with low variance. Also reading as F64 before scaling.
0.30840

LEAP with feature engineering - Version 17
Complete · 9h ago · commit a34b29920 Going back to RMS y-normalisation in case change there somehow responsible for bad recent scores
-1.32174

LEAP with feature engineering - Version 16
Complete · 3m ago · Commit 7cef81edd SiLU activation, allowing larger 'good' values in columns
-2.34299

LEAP with feature engineering - Version 15 *** now using 30,000 training rows ***
Complete · 3m ago · Commit 3765366 zeroing crazy large-number cols but not zeroing out all 'low variance' ones
-0.39082

LEAP with feature engineering - Version 12  *** up to this point using 100,000 training rows ***
Complete · 2m ago · commit 265c9f3bac not zeroing out invariant result columns
-1037770694134379085769163933486585061126756728541364533486745413084161537962122674176.00000

LEAP with feature engineering - Version 11
Complete · 2m ago · commit 67fdcab66 with reciprocal density
0.34119

LEAP with feature engineering - Version 10
Complete · 3m ago · Commit 0d8d069 removed a few raw features
0.34286

LEAP with feature engineering - Version 9
Complete · 2m ago · commit fd0d97d93, first with relative humidity
0.26862

LEAP with feature engineering - Version 7
Complete · 4m ago · commit 34faf3590 again but with new features turned off for comparison
0.22994

LEAP with feature engineering - Version 6
Complete · 42m ago · commit 34faf35, 1000000 training rows failed I think, this is 100000
0.25368

LEAP with feature engineering - Version 4
Complete · 11h ago · f2a52c8 First attempts with some feature engineering on limited number of rows
0.23289
