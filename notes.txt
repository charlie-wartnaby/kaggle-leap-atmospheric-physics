Feature engineering
-------------------
Would like to derive variables e.g. density, momentum from known data
See load-grid-info.py to get pressures at altitude levels

# Altitude levels in hPa from ClimSim-main\grid_info\ClimSim_low-res_grid-info.nc
level_pressure_hpa = [0.07834781133863082, 0.1411083184744011, 0.2529232969453412, 0.4492506351686618, 0.7863461614709879, 1.3473557602677517, 2.244777286900205, 3.6164314830257718, 5.615836425337344, 8.403253219853443, 12.144489352066294, 17.016828024303006, 23.21079811610005, 30.914346261995327, 40.277580662953575, 51.37463234765765, 64.18922841394662, 78.63965761131159, 94.63009200213703, 112.09127353988006, 130.97780378937776, 151.22131809551237, 172.67390465199267, 195.08770981962772, 218.15593476138105, 241.60037901222947, 265.2585152868483, 289.12232222921756, 313.31208711045167, 338.0069992368819, 363.37349177951705, 389.5233382784413, 416.5079218282233, 444.3314120123719, 472.9572063769364, 502.2919169181905, 532.1522731583445, 562.2393924639011, 592.1492760575118, 621.4328411158061, 649.689897132655, 676.6564846051039, 702.2421877859194, 726.4985894989197, 749.5376452869328, 771.4452171682528, 792.2342599534793, 811.8566751313328, 830.2596431972574, 847.4506530638328, 863.5359020075301, 878.7158746040692, 893.2460179738746, 907.3852125876941, 921.3543974831824, 935.3167171670306, 949.3780562075774, 963.5995994020714, 978.013432382012, 992.6355435925217]

Tips from others
----------------

Polars found to be much faster than pandas in saving very large .csv (though not
clear if forming dataframe included):
https://www.kaggle.com/competitions/leap-atmospheric-physics-ai-climsim/discussion/495128

Nice simple example code showing PyTorch feedforward model and submission:
https://www.kaggle.com/code/airazusta014/pytorch-nn
Another nice simple NN using Keras:
https://www.kaggle.com/code/ymatioun/leap-simple-nn

Reduced-size dataset in parquet format, but says features in float16 format yet many
feature values are well below minimum value for float16 so information will be lost:
https://www.kaggle.com/datasets/titericz/leap-dataset-giba

Dataset size, processing all with modest memory
-----------------------------------------------
https://www.kaggle.com/code/charliewartnaby/big-dataframe-processing
Using pl.scan_csv(), lazy_df.select(pl.len()).collect().item() :
Reading /kaggle/input/leap-atmospheric-physics-ai-climsim/test.csv
Length:  625000 Seconds:  25.933050632476807
Reading /kaggle/input/leap-atmospheric-physics-ai-climsim/sample_submission.csv
Length:  625000 Seconds:  13.82296085357666
Reading /kaggle/input/leap-atmospheric-physics-ai-climsim/train.csv
Length:  10091520 Seconds:  798.5059781074524

Train.csv has 924 numeric columns so ~9e9 numbers, ~35 GB as float32

Do I need to do something custom (caching knowledge of byte offsets of line endings)
or can Polars.LazyFrame do this anyway?
Need to interate feature engineering in to Dataset __getitem__ when accessing
efficiently from disk?
Maybe just do some timing experiments trying to read rows from large file after
Polars has had chance to map the file
polars.LazyFrame.gather_every() maybe useful for train/validation split?

Using polars.LazyFrame.slice() on 625000 row test.csv file with 3 rows:
Materalising from offset: 62500 proportion: 0.1 ... took 9.378756523132324
Materalising from offset: 562500 proportion: 0.9 ... took 73.70131516456604
Materalising from offset: 125000 proportion: 0.2 ... took 1.3532490730285645
Materalising from offset: 312500 proportion: 0.5 ... took 3.406555652618408
Materalising from offset: 593750 proportion: 0.95 ... took 10.94671368598938
(similar numbers if materialising 1000 rows instead of 3)
-- so certainly much quicker once it had been forced to access near the end of the
file; but stilll slow. Could be OK if taking a decent batch of rows each time
so overhead per row not so bad, but not if getting one at a time.
If do pl.len().collect() first, that took 28 sec and then fetch to 0.9* file another
6.5s, so that was a bit quicker (oddly). (Did stop/start session to avoid
RAM caching.)
Found materialising block of rows near end of test.csv using Polars having got length of
file took so long I abandoned it (>1 hr on Kaggle)
Indexing full training file myself by line took 41 mins but could then access a
random row in 2.5 ms using file.seek. So will do my own thing.

Created dataset with my row byte offsets:
https://www.kaggle.com/datasets/charliewartnaby/leap-atmospheric-physics-file-row-offsets

Ideas to progress
-----------------

o Figure out faster/cheaper validation test to avoid using GPU quota and quicker turnaround
o Understand physics of each target col better
o Does humidity have significant effect on heat capacity? And cloud/rain/snow content?
o Reciprocal of new features, e.g. heating in K/s will be inversely proportional to density
 for given heat input
o Deeper/alternative network structures
o CNN-type network that scans across altitude levels (mixed with those above and below)
o Batch-based training on whole dataset or much bigger subset
o Try other ML techniques apart from NN
o Would it help to remove more old feature columns?
o Would it help to group columns differently depending on technique?
o If big run dominated by data loading, could train different model variants simultaneously?


Scoring and why predictions zeroed for low-variance cols in example
-------------------------------------------------------------------

Uses sklearn.metrics.r2_score
Competition links to https://www.kaggle.com/code/jerrylin96/r2-score-default
Discussion https://www.kaggle.com/competitions/leap-atmospheric-physics-ai-climsim/discussion/495255
R2 is average across R2 for individual columns, but with weighting
R2 for one col = 1 - ( (sq resids sum of prediction) / ((true - avg true) sq sum))
So get R2=0 if just guess mean for column
If true values have very low variance, (true - avg true) -> 0 and R2 -> inf
R2=0 a lot better than wild value if a little bit off for low-variance columns, hence
guessing mean like sample code


Beucler et al
o Transformed relative humidity to specific humidity for much better generalisation
  https://colab.research.google.com/github/tbeucler/CBRAIN-CAM/blob/master/Climate_Invariant_Guide.ipynb
o Transformed temperature to a buoyancy metric, also improved but only with RH too
o Radiative effects: absorbtion of SW (visible), greenhouse trapping of LW (IR)
o Sensible heat flux: +ve from atm to surface (radiation, conduction, convection??)
o Latent heat flux: +ve for condensation on surface, -ve for evaporation from surface


Results from early expts
------------------------

Showed that it did a touch better with additional new features, and getting rid of some raw ones:

LEAP with feature engineering - Version 19
Complete · 4m ago · commit afda421d20 Multiplying by submission weights before modelling now
0.32041

LEAP with feature engineering - Version 18
Complete · 19h ago · commit c7b0cfa660 Understood R2 measure now and some sense of why sample code was zeroing (actually setting mean) for columns with low variance. Also reading as F64 before scaling.
0.30840

LEAP with feature engineering - Version 17
Complete · 9h ago · commit a34b29920 Going back to RMS y-normalisation in case change there somehow responsible for bad recent scores
-1.32174

LEAP with feature engineering - Version 16
Complete · 3m ago · Commit 7cef81edd SiLU activation, allowing larger 'good' values in columns
-2.34299

LEAP with feature engineering - Version 15 *** now using 30,000 training rows ***
Complete · 3m ago · Commit 3765366 zeroing crazy large-number cols but not zeroing out all 'low variance' ones
-0.39082

LEAP with feature engineering - Version 12  *** up to this point using 100,000 training rows ***
Complete · 2m ago · commit 265c9f3bac not zeroing out invariant result columns
-1037770694134379085769163933486585061126756728541364533486745413084161537962122674176.00000

LEAP with feature engineering - Version 11
Complete · 2m ago · commit 67fdcab66 with reciprocal density
0.34119

LEAP with feature engineering - Version 10
Complete · 3m ago · Commit 0d8d069 removed a few raw features
0.34286

LEAP with feature engineering - Version 9
Complete · 2m ago · commit fd0d97d93, first with relative humidity
0.26862

LEAP with feature engineering - Version 7
Complete · 4m ago · commit 34faf3590 again but with new features turned off for comparison
0.22994

LEAP with feature engineering - Version 6
Complete · 42m ago · commit 34faf35, 1000000 training rows failed I think, this is 100000
0.25368

LEAP with feature engineering - Version 4
Complete · 11h ago · f2a52c8 First attempts with some feature engineering on limited number of rows
0.23289