Feature engineering
-------------------
Would like to derive variables e.g. density, momentum from known data
See load-grid-info.py to get pressures at altitude levels

# Altitude levels in hPa from ClimSim-main\grid_info\ClimSim_low-res_grid-info.nc
level_pressure_hpa = [0.07834781133863082, 0.1411083184744011, 0.2529232969453412, 0.4492506351686618, 0.7863461614709879, 1.3473557602677517, 2.244777286900205, 3.6164314830257718, 5.615836425337344, 8.403253219853443, 12.144489352066294, 17.016828024303006, 23.21079811610005, 30.914346261995327, 40.277580662953575, 51.37463234765765, 64.18922841394662, 78.63965761131159, 94.63009200213703, 112.09127353988006, 130.97780378937776, 151.22131809551237, 172.67390465199267, 195.08770981962772, 218.15593476138105, 241.60037901222947, 265.2585152868483, 289.12232222921756, 313.31208711045167, 338.0069992368819, 363.37349177951705, 389.5233382784413, 416.5079218282233, 444.3314120123719, 472.9572063769364, 502.2919169181905, 532.1522731583445, 562.2393924639011, 592.1492760575118, 621.4328411158061, 649.689897132655, 676.6564846051039, 702.2421877859194, 726.4985894989197, 749.5376452869328, 771.4452171682528, 792.2342599534793, 811.8566751313328, 830.2596431972574, 847.4506530638328, 863.5359020075301, 878.7158746040692, 893.2460179738746, 907.3852125876941, 921.3543974831824, 935.3167171670306, 949.3780562075774, 963.5995994020714, 978.013432382012, 992.6355435925217]

Ideas to progress
-----------------

o Haven't explored optimiser etc (but did learning rate)
o Deal properly with columns that are going to be zero anyway
o Understand physics of each target col better
o Does humidity have significant effect on heat capacity? And cloud/rain/snow content?
o Reciprocal of new features, e.g. heating in K/s will be inversely proportional to density
 for given heat input
o Deeper/alternative network structures
o Try other ML techniques apart from NN
o Would it help to remove more old feature columns?
o Would it help to group columns differently depending on technique?


Scoring and why predictions zeroed for low-variance cols in example
-------------------------------------------------------------------

Uses sklearn.metrics.r2_score
Competition links to https://www.kaggle.com/code/jerrylin96/r2-score-default
Discussion https://www.kaggle.com/competitions/leap-atmospheric-physics-ai-climsim/discussion/495255
R2 is average across R2 for individual columns, but with weighting
R2 for one col = 1 - ( (sq resids sum of prediction) / ((true - avg true) sq sum))
So get R2=0 if just guess mean for column
If true values have very low variance, (true - avg true) -> 0 and R2 -> inf
R2=0 a lot better than wild value if a little bit off for low-variance columns, hence
guessing mean like sample code


Beucler et al
o Transformed relative humidity to specific humidity for much better generalisation
  https://colab.research.google.com/github/tbeucler/CBRAIN-CAM/blob/master/Climate_Invariant_Guide.ipynb
o Transformed temperature to a buoyancy metric, also improved but only with RH too
  See "Source code for the moist thermodynamics library" cell, class T2BMSENumpy:
  https://colab.research.google.com/github/tbeucler/CBRAIN-CAM/blob/master/Climate_Invariant_Guide.ipynb#scrollTo=0S6W988UaG6p
o Radiative effects: absorbtion of SW (visible), greenhouse trapping of LW (IR)
o Sensible heat flux: +ve from atm to surface (radiation, conduction, convection??)
o Latent heat flux: +ve for condensation on surface, -ve for evaporation from surface

Performance
-----------

At a17aae4 on greta scaling and model saved at 03:10 first cache 11:55 so ~3hr15 for 20 epochs, batch size 30000
On pcuk151 first cache 21:12, epoch 10 finishing 05:50 so ~8hr40 for 10 epochs, batch size 5000
Though validation loss looking better so far on pcuk151, maybe batches too big?

Submission Results
------------------

submission.csv
Complete · 5m ago · Commit b8a8d4ec 4th conv layer all depth 3, 30000s
0.60257

submission-bd3a254.csv
Complete · 5m ago · Commit bd3a254 more depth in conv layers, 5000-row batches
0.62144 (notably better than larger batches this time at least)

submission.csv
Complete · 9h ago · Commit a17aae411 deeper conv layers 30000 batches
0.60331

submission-1b655d19.csv
Complete · 2h ago · Commit 1b655d19 with experimental no centring of x values in normalisation either
0.59778 -- so a bit worse overall, will centre x values again


submission.csv
Complete · 2m ago · commit 9ccabc17 not centering Y values in normalisation
0.60646

submission-5a5069fa.csv
Complete · 6m ago · Commit 5a5069fa 10 epochs with high dropout proportion, validation loss not good though
0.55404


Tried LR=0.01 but didn't really converge:
	Epoch 1, Step 610, Training Loss: 0.6596
With LR=0.004 seemed very smooth, actually slower than 0.001:
Epoch 2, Step 890, Training Loss: 0.4715
Trying now with LR=0.0004 on Kaggle for 3 epochs:
Epoch 3, Validation Loss: 0.4196762824707692 so no improvement
So default learning rate of 0.001 actually good with current setup.


commit 25b97f1f Fix to apparently incorrect y scaling
Oddly didn't help, maybe worse -- maybe means scaling alone is fine, no need
to subtract mean at all, which would save time.
Epoch 10, Validation Loss: 0.38333882804554287 (was 0.367 below before fix)
Epoch 13, Validation Loss: 0.38022170975656794 + 3 epochs for sanity check submit
Epoch 15, Validation Loss: 0.3789655051314005
Epoch 18, Validation Loss: 0.37621909586509855
Did end up with best score so far, but a lot of epochs:
submission-25b97f1f-23-epochs.csv
Complete · 2h ago · commit 25b97f1f With y scaling fixed (but didn't help?)
0.59192

submission-559738c.csv
Complete · 3m ago · Normalising inputs (but not yet outputs) across whole channels
0.58698
Epoch 1, Validation Loss: 0.43317776520063384
Epoch 2, Validation Loss: 0.41231624118172294
Epoch 3, Validation Loss: 0.39903453848149517
Epoch 4, Validation Loss: 0.3914585606296464
Epoch 5, Validation Loss: 0.38657170696423787 
Epoch 6, Validation Loss: 0.38171850779269
Epoch 7, Validation Loss: 0.3780829362054863
Epoch 8, Validation Loss: 0.3759397563367787
Epoch 9, Validation Loss: 0.37318830088813704
Epoch 10, Validation Loss: 0.3713240346105972
Epoch 11, Validation Loss: 0.36997252392886887
Epoch 12, Validation Loss: 0.36848068200420625
Epoch 13, Validation Loss: 0.3670755941236373

submission-763ae6b9.csv
Complete · 4m ago · Commit 763ae6b9 Trying more channels in early CNN layers
0.57444
Epoch 8, Validation Loss: 0.3771515665668072
Epoch 9, Validation Loss: 0.37443682448108595
Epoch 10, Validation Loss: 0.37198687337412695


submission_8531e050.csv
Complete · 6m ago · Commit 8531e050 with more droput and layer normalisations
Epoch 19, Validation Loss: 0.38724345529433524 (full dataset)
0.56218

submission.csv
Complete · 2m ago · Commit 77a26e95b9a local run to validation score ~0.4 on full dataset with CNN
0.53044

LEAP with feature engineering - Version 35
Complete · 3m ago · Commit 2c9a279 smallish test run with CNN approach and overhauled test output
With only: Validation Loss: 0.5902572572231293
0.35463

LEAP with feature engineering - Version 25
Complete · 3d ago · commit 755728af first test run with holo frame concept
0.28431

LEAP with feature engineering - Version 19
Complete · 4m ago · commit afda421d20 Multiplying by submission weights before modelling now
0.32041

LEAP with feature engineering - Version 18
Complete · 19h ago · commit c7b0cfa660 Understood R2 measure now and some sense of why sample code was zeroing (actually setting mean) for columns with low variance. Also reading as F64 before scaling.
0.30840

LEAP with feature engineering - Version 17
Complete · 9h ago · commit a34b29920 Going back to RMS y-normalisation in case change there somehow responsible for bad recent scores
-1.32174

LEAP with feature engineering - Version 16
Complete · 3m ago · Commit 7cef81edd SiLU activation, allowing larger 'good' values in columns
-2.34299

LEAP with feature engineering - Version 15 *** now using 30,000 training rows ***
Complete · 3m ago · Commit 3765366 zeroing crazy large-number cols but not zeroing out all 'low variance' ones
-0.39082

LEAP with feature engineering - Version 12  *** up to this point using 100,000 training rows ***
Complete · 2m ago · commit 265c9f3bac not zeroing out invariant result columns
-1037770694134379085769163933486585061126756728541364533486745413084161537962122674176.00000

LEAP with feature engineering - Version 11
Complete · 2m ago · commit 67fdcab66 with reciprocal density
0.34119

LEAP with feature engineering - Version 10
Complete · 3m ago · Commit 0d8d069 removed a few raw features
0.34286

LEAP with feature engineering - Version 9
Complete · 2m ago · commit fd0d97d93, first with relative humidity
0.26862

LEAP with feature engineering - Version 7
Complete · 4m ago · commit 34faf3590 again but with new features turned off for comparison
0.22994

LEAP with feature engineering - Version 6
Complete · 42m ago · commit 34faf35, 1000000 training rows failed I think, this is 100000
0.25368

LEAP with feature engineering - Version 4
Complete · 11h ago · f2a52c8 First attempts with some feature engineering on limited number of rows
0.23289