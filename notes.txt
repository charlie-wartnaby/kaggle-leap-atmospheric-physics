Ideas to progress
-----------------

o Bug if test only currently does validation loops
o Logarithmic scaling so that reasonable values near 1, extremes not so extreme?
o Went badly deleting hopefully redundant features at b32db7d49, so what is so crucial?
o Analyse which columns contributing most to error metric:
   - ptend_q0001 moistening tendency low indices >=12, then higher ~45
   - ptend_q0002 cloud liquid/ice mixing ratios and ptend_q0002 ice mixing ratios
   -- both to do with condensation/evaporation
   -- mass mixing ratio is m(ice/water) / m(dry air) 
o Prev expts where knocked out half featurs etc bogus because before new ones added
o Haven't explored optimiser etc (but did learning rate)
o Deal properly with columns that are going to be zero anyway
o Try range instead of stdev scaling again now range calc fixed
  - if using range scaling need to still compute stdev for R2 metric
o Tried float64 on 1M samples, very slow and convergence oddly seemed worse
o Understand physics of each target col better
o Does humidity have significant effect on heat capacity? And cloud/rain/snow content?
o Deeper/alternative network structures
o Would it help to group columns differently depending on technique?
o Train/val split does appear deterministic (random seeds set)
o Would catboost work for more iterations with smaller batches (thought not could revisit)
o When training subset (e.g. for catboost output features) should draw random samples
  from whole training dataset, not one clump

Scoring and why predictions zeroed for low-variance cols in example
-------------------------------------------------------------------

Uses sklearn.metrics.r2_score
Competition links to https://www.kaggle.com/code/jerrylin96/r2-score-default
Discussion https://www.kaggle.com/competitions/leap-atmospheric-physics-ai-climsim/discussion/495255
R2 is average across R2 for individual columns, but with weighting
R2 for one col = 1 - ( (sq resids sum of prediction) / ((true - avg true) sq sum))
So get R2=0 if just guess mean for column
If true values have very low variance, (true - avg true) -> 0 and R2 -> inf
R2=0 a lot better than wild value if a little bit off for low-variance columns, hence
guessing mean like sample code


Beucler et al
o Transformed relative humidity to specific humidity for much better generalisation
  https://colab.research.google.com/github/tbeucler/CBRAIN-CAM/blob/master/Climate_Invariant_Guide.ipynb
o Transformed temperature to a buoyancy metric, also improved but only with RH too
  See "Source code for the moist thermodynamics library" cell, class T2BMSENumpy:
  https://colab.research.google.com/github/tbeucler/CBRAIN-CAM/blob/master/Climate_Invariant_Guide.ipynb#scrollTo=0S6W988UaG6p
o Radiative effects: absorbtion of SW (visible), greenhouse trapping of LW (IR)
o Sensible heat flux: +ve from atm to surface (radiation, conduction, convection??)
o Latent heat flux: +ve for condensation on surface, -ve for evaporation from surface

Performance
-----------

Found layer (not batch) normalisation and SiLU best in experiment 26May2024 though little in it

Submission Results
------------------

submission.csv
Complete · 3m ago · Commit f9266af6 another 6 epochs on 2024_07_07_71741533_full_cnn_3_midlayers_late_catboost now with Hu ice/water cloud partitioning, but looks worse local R2=0.719
0.70814

submission.csv
Complete · 7m ago · Commit 2c2c3bb18 comparison CNN without catboost feature injection, ended after 22 epochs as not improving
0.71781

submission.csv
Complete · 3m ago · Commit 71741533 with 3 CNN midlayers and late catboost features, 10M rows, 30 epochs, local R2=0.733
0.72630

submission.csv - actually no better than 400 iterations
Complete · 4m ago · Commit a86308a9 1M catboost rows offset from 8M, with 499 iterations instead of 400, to hopefully build better CNN input features
0.44262

submission.csv
Complete · 2h ago · Commit 74824dca additional push with last dregs of training data, total 29 epochs
0.72527

submission.csv
Complete · 6m ago · Commit 5ff4fbec3 CNN with late injection of catboost outputs, full 10M rows, first 20 epochs, best local R2=0.725
0.71048

submission.csv
Complete · 4m ago · Commit 917f41b3 full CNN run with catboost output features as early inputs, not looking any better, 24 epochs final validation R2=0.726
0.71349

submission.csv
Complete · 19m ago · Commit ace4e2fa modest 300K rows catboost, wasn't intending to submit but local R2=0.566 looking oddly better
0.44540

submission.csv
Complete · 11h ago · Commit 8a50b544 current CNN over all 10M rows resumed and left for 33 epochs, validation R2=0.729
0.71705

submission.csv
Complete · 1d ago · Commit 6b46658 with R2^4-weighted CNN and catboost contributions
0.68357

submission.csv
Complete · 1d ago · Commit b713d09 choosing max R2 CNN and catboost contributions
0.68306

submission.csv
Complete · 1d ago · Commit 1dbed19fb with squared R2-weighted CNN and catboost contributions
0.67852

submission.csv
Complete · 1d ago · Commit 2d3f0e47 with linearly R2-weighted CNN and catboost contributions
0.66874

submission.csv
Complete · 2h ago · Commit 867b3ea4a reference catboost run with 1M rows
0.44679

submission.csv
Complete · 4m ago · Commit a9baf2a 1M row catboost sanity check run but inadvertently left batch size small at 5K
0.32939

submission.csv
Complete · 3m ago · Commit fa7e995a8 CNN baseline on all 10M rows only 7 epochs so far and not converged yet, submitting as sanity check
0.69652

Hallejulah, back in the game
submission.csv
Complete · 5m ago · Commit 164eb773f same model overtrained mainly on 100K rows but with fixes to avg substitutions
0.65339

Gah what have I done?
submission.csv
Complete · 3m ago · Commit ce48b16 R2 validation stats looking sensible now, trying 1 epoch 1M rows having done ~42 in total with only 100K so prob overtrained on that subset
-28480891028409900.00000

submission.csv
Complete · 6h ago · Commit 7598cc73 test with 1M rows after restored use of old weightings and cloud tricks, but local R2 values look like nonesense so probably no good
0.10532

Still in trouble...
submission.csv
Complete · 4m ago · Commit 3628ba9 CNN contd with only 1M rows, min_std=1e-10 now
-1146.31352

submission.csv
Complete · 5m ago · Commit b6c560a9 CNN big run but 20 epochs old weightings, to 31 epochs with new weightings hoping would transfer, R2 locally looks terrible though validation loss OK
-392.89850

Hmm don't know what went so terribly wrong here...
submission.csv
Complete · 3m ago · Commit b6c560a9 switched to new submission weights for only last 4 of 20 epochs, baseline CNN rerun to check not broken after refactor
-30331453942559299857621134082048.00000

averaged_submission.csv
Complete · 5m ago · Commit 15a6f84 Experimentally averaging 5 individual submission dataframes over first 500K rows to compare with catboost model sum over same data
0.37763

submission.csv
Complete · 26m ago · Commit 02a87b6 catboost over 500K rows but data normalised over 5M
0.39272

submission.csv
Complete · 23m ago · Commit 718c237 catboost over first 100K rows but data normalised over 5M
0.35673

submission.csv
Complete · 4m ago · Commit c6093c7b best catboost params on 2M [actually 200K] rows but looks worse than 100K expts for some reason
0.36803

submission.csv -- doing better with more iterations and higher learning rate
Complete · 4h ago · Commit 63d2cf47 multitrain hyperparam search with catboost, ended using model_border_count_16_depth_8_iterations_20_learning_rate_0.1_l2_leaf_reg_2.pkl
0.30880

submission.csv -- hmm slightly worse doing 10M not 1M training rows... many failed R2 test in fact.
Complete · 4m ago · Commit f2bc0a9 all-rows catboost run after R2<=0 fix
0.22795

submission.csv
Complete · 2m ago · Commit 0d79ad2 catboost test run with 1M rows after R2<=0 exclusion bugfix
0.23663

Oops what went wrong with this?
submission.csv
Complete · 3m ago · Commit 7190f38 catboost with more features and all training rows, substituting mean where R2 <= 0
-875470.00698

submission.csv
Complete · 6m ago · Commit 0b349a25 first catboost test with only 10000 rows
0.17413

submission.csv
Complete · 3m ago · Commit 20fe5c45b trying a simpler feature set and model on a hunch in case converged better, still with poly output layer, but val loss doesn't look good
0.59769

submission.csv
Complete · 2h ago · Commit f432f9e poly output again but poor val loss
0.62373 (better than I expected)

submission.csv
Complete · 7m ago · Commit 18298fd polynomial output to try and reach outlier values but didn't converge as well as in smaller run
0.55093

Better validation loss here, but no better in competition:
submission.csv
Complete · 2m ago · Commit d138788 with cloud mass and rethought GWP features 26 epochs final val loss ~0.275
0.66457

submission.csv
Complete · 4m ago · Commit 72fb1b5 continued now with uniform scaling across atmospheric levels
0.67054

submission.csv
Complete · 1d ago · Commit d0733e88 no y norm offset, a few cols now nixed
0.66888

submission.csv
Complete · 6m ago · Commit 77a04229 with integral cloud features
0.65123
Ran until patience counter of 3 exhausted, 35 epochs validation loss 0.31098 best

submission.csv
Complete · 6m ago · Commit b1472343 with corrected width 1-deep initial conv
Ran until patience counter of 3 exhausted, 42 epochs validation loss 0.31218 beset
0.65009

submission.csv
Complete · 4m ago · Commit e352323c with new scalar/vector output structure and bigger conv depth, 23 epochs
0.64229

submission.csv
Complete · 2h ago · Commit e77e89d8 first automated model param search, only 10 epochs, best of 9 was model.pt_gen_conv_width_7_gen_conv_depth_12.pt
In general depth in conv layers was dominant, 12 better than 6 better than 3.
Width across atm layers less important, but probably 7/15 better than 3.
But also a lot more parameters and computing time.
0.61821


submission.csv
Complete · 19m ago · Commit 939c2b5 reverting to fewer but deeper conv layers, 5000 batches, with buoyancy feature
0.62494

submission.csv
Complete · 5m ago · Commit b8a8d4ec 4th conv layer all depth 3, 30000s
0.60257

submission-bd3a254.csv
Complete · 5m ago · Commit bd3a254 more depth in conv layers, 5000-row batches
0.62144 (notably better than larger batches this time at least)

submission.csv
Complete · 9h ago · Commit a17aae411 deeper conv layers 30000 batches
0.60331

submission-1b655d19.csv
Complete · 2h ago · Commit 1b655d19 with experimental no centring of x values in normalisation either
0.59778 -- so a bit worse overall, will centre x values again


submission.csv
Complete · 2m ago · commit 9ccabc17 not centering Y values in normalisation
0.60646

submission-5a5069fa.csv
Complete · 6m ago · Commit 5a5069fa 10 epochs with high dropout proportion, validation loss not good though
0.55404


Tried LR=0.01 but didn't really converge:
	Epoch 1, Step 610, Training Loss: 0.6596
With LR=0.004 seemed very smooth, actually slower than 0.001:
Epoch 2, Step 890, Training Loss: 0.4715
Trying now with LR=0.0004 on Kaggle for 3 epochs:
Epoch 3, Validation Loss: 0.4196762824707692 so no improvement
So default learning rate of 0.001 actually good with current setup.


commit 25b97f1f Fix to apparently incorrect y scaling
Oddly didn't help, maybe worse -- maybe means scaling alone is fine, no need
to subtract mean at all, which would save time.
Epoch 10, Validation Loss: 0.38333882804554287 (was 0.367 below before fix)
Epoch 13, Validation Loss: 0.38022170975656794 + 3 epochs for sanity check submit
Epoch 15, Validation Loss: 0.3789655051314005
Epoch 18, Validation Loss: 0.37621909586509855
Did end up with best score so far, but a lot of epochs:
submission-25b97f1f-23-epochs.csv
Complete · 2h ago · commit 25b97f1f With y scaling fixed (but didn't help?)
0.59192

submission-559738c.csv
Complete · 3m ago · Normalising inputs (but not yet outputs) across whole channels
0.58698
Epoch 1, Validation Loss: 0.43317776520063384
Epoch 2, Validation Loss: 0.41231624118172294
Epoch 3, Validation Loss: 0.39903453848149517
Epoch 4, Validation Loss: 0.3914585606296464
Epoch 5, Validation Loss: 0.38657170696423787 
Epoch 6, Validation Loss: 0.38171850779269
Epoch 7, Validation Loss: 0.3780829362054863
Epoch 8, Validation Loss: 0.3759397563367787
Epoch 9, Validation Loss: 0.37318830088813704
Epoch 10, Validation Loss: 0.3713240346105972
Epoch 11, Validation Loss: 0.36997252392886887
Epoch 12, Validation Loss: 0.36848068200420625
Epoch 13, Validation Loss: 0.3670755941236373

submission-763ae6b9.csv
Complete · 4m ago · Commit 763ae6b9 Trying more channels in early CNN layers
0.57444
Epoch 8, Validation Loss: 0.3771515665668072
Epoch 9, Validation Loss: 0.37443682448108595
Epoch 10, Validation Loss: 0.37198687337412695


submission_8531e050.csv
Complete · 6m ago · Commit 8531e050 with more droput and layer normalisations
Epoch 19, Validation Loss: 0.38724345529433524 (full dataset)
0.56218

submission.csv
Complete · 2m ago · Commit 77a26e95b9a local run to validation score ~0.4 on full dataset with CNN
0.53044

LEAP with feature engineering - Version 35
Complete · 3m ago · Commit 2c9a279 smallish test run with CNN approach and overhauled test output
With only: Validation Loss: 0.5902572572231293
0.35463

LEAP with feature engineering - Version 25
Complete · 3d ago · commit 755728af first test run with holo frame concept
0.28431

LEAP with feature engineering - Version 19
Complete · 4m ago · commit afda421d20 Multiplying by submission weights before modelling now
0.32041

LEAP with feature engineering - Version 18
Complete · 19h ago · commit c7b0cfa660 Understood R2 measure now and some sense of why sample code was zeroing (actually setting mean) for columns with low variance. Also reading as F64 before scaling.
0.30840

LEAP with feature engineering - Version 17
Complete · 9h ago · commit a34b29920 Going back to RMS y-normalisation in case change there somehow responsible for bad recent scores
-1.32174

LEAP with feature engineering - Version 16
Complete · 3m ago · Commit 7cef81edd SiLU activation, allowing larger 'good' values in columns
-2.34299

LEAP with feature engineering - Version 15 *** now using 30,000 training rows ***
Complete · 3m ago · Commit 3765366 zeroing crazy large-number cols but not zeroing out all 'low variance' ones
-0.39082

LEAP with feature engineering - Version 12  *** up to this point using 100,000 training rows ***
Complete · 2m ago · commit 265c9f3bac not zeroing out invariant result columns
-1037770694134379085769163933486585061126756728541364533486745413084161537962122674176.00000

LEAP with feature engineering - Version 11
Complete · 2m ago · commit 67fdcab66 with reciprocal density
0.34119

LEAP with feature engineering - Version 10
Complete · 3m ago · Commit 0d8d069 removed a few raw features
0.34286

LEAP with feature engineering - Version 9
Complete · 2m ago · commit fd0d97d93, first with relative humidity
0.26862

LEAP with feature engineering - Version 7
Complete · 4m ago · commit 34faf3590 again but with new features turned off for comparison
0.22994

LEAP with feature engineering - Version 6
Complete · 42m ago · commit 34faf35, 1000000 training rows failed I think, this is 100000
0.25368

LEAP with feature engineering - Version 4
Complete · 11h ago · f2a52c8 First attempts with some feature engineering on limited number of rows
0.23289
